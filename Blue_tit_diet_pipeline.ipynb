{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Blue tit nestling diet pipeline\n",
        "#### Emma Poliakova \n",
        "LICENSE: https://github.com/EmmaPoliakova/BlueTitDiet/blob/main/LICENSE \n",
        "\n",
        "<br> \n",
        "<br>\n",
        "This notebook is for processing blue tit nest box recordings. \n",
        "\n",
        "### To get started: \n",
        "\n",
        "* in the meno on top of the page click file -> save a copy in Drive \n",
        "\n",
        "* download the zip file with pretrained models for blue tit detection, landmark localization, and food classifier from here:https://www.mediafire.com/file/l8p2mum2jv3ofva/BlueTitDiet.zip/file\n",
        "\n",
        "* upload it to the landing page of your google drive\n",
        "\n",
        "* open the runtime section on the top of the page, select change runtime type and select GPU\n",
        "\n",
        "* run the imports and setup code sections\n",
        "\n",
        "* if no errors show you are ready to process the videos"
      ],
      "metadata": {
        "id": "VP9m8R9AEDrc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD0j6SPQtVcV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "UJLH_kLt9Cg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb87bdc-b135-43d8-e205-b14f7d0822c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLt529F0tCjG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "\n",
        "import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as nnf\n",
        "import torch.optim as optim\n",
        "from os import path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SetUp\n",
        "\n",
        "You only need to run this section when you are running this notebook for the first time."
      ],
      "metadata": {
        "id": "fMpAGAFR0Wdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./gdrive/MyDrive/BlueTitDiet.zip"
      ],
      "metadata": {
        "id": "Xt2lHahbwZ6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5 \n",
        "!pip install -U -r yolov5/requirements.txt "
      ],
      "metadata": {
        "id": "206rinLt4j65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ./yolov5/detect.py\n",
        "!cp ./BlueTitDiet/code/detect.py ./yolov5/\n",
        "if path.exists('./yolov5/runs/detect/') == False:\n",
        "  os.mkdir('./yolov5/runs')\n",
        "  os.mkdir('./yolov5/runs/detect')\n",
        "  os.mkdir('./yolov5/runs/detect/pictures')\n",
        "  os.mkdir('./yolov5/runs/detect/crops')\n"
      ],
      "metadata": {
        "id": "WifDULb_10K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jR6EoRGt16T"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.8.2\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0466mkNUt6HG"
      },
      "outputs": [],
      "source": [
        "!rm ./coco_eval.py\n",
        "!cp ./BlueTitDiet/code/coco_eval.py ./\n",
        "\n",
        "from engine import train_one_epoch, evaluate\n",
        "import transforms, utils, engine\n",
        "from utils import collate_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "ddM24lKG0dW6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64jvgro9tmfj"
      },
      "outputs": [],
      "source": [
        "def get_model(num_keypoints, weights_path=None):\n",
        "    \n",
        "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
        "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
        "                                                                   pretrained_backbone=True,\n",
        "                                                                   num_keypoints=num_keypoints,\n",
        "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
        "                                                                   rpn_anchor_generator=anchor_generator)\n",
        "\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)        \n",
        "        \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rAnzebwuLmA"
      },
      "outputs": [],
      "source": [
        "# training and prediction keypoint rcnn code modified from [2]\n",
        "def crop_images(in_folder, out_folder, width, height):\n",
        "  \n",
        "  width_half = width//2\n",
        "\n",
        "  for img in os.listdir(in_folder):\n",
        "    if img.endswith(\".png\"):\n",
        "      images = []\n",
        "      path = in_folder + img\n",
        "      img_original = cv2.imread(path)\n",
        "      images.append(F.to_tensor(img_original))\n",
        "\n",
        "      images = list(image.to(device) for image in images)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          model.to(device)\n",
        "          model.eval()\n",
        "          output = model(images)\n",
        "\n",
        "      image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
        "      scores = output[0]['scores'].detach().cpu().numpy()\n",
        "\n",
        "      high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
        "      post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() \n",
        "\n",
        "      keypoints = []\n",
        "      for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
        "          keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
        "\n",
        "      if keypoints:\n",
        "        beak = keypoints[0][-1]\n",
        "        cropped_image =  img_original[ beak[1] : beak[1] + height , beak[0] - width_half: beak[0] + width_half]\n",
        "\n",
        "        path_crop = out_folder + img \n",
        "        cv2.imwrite(path_crop, cropped_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6_BrSDC654L"
      },
      "outputs": [],
      "source": [
        "def most_common(lst):\n",
        "    return max(set(lst), key=lst.count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0lB-Jk65JSp"
      },
      "outputs": [],
      "source": [
        "#implementation by Balawejder, M. [3]\n",
        "class ClassDataset(Dataset):\n",
        "    def __init__(self, image_paths, dataset, location, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = dataset\n",
        "        self.transform = transform\n",
        "        self.location = location\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def get_labels(self):\n",
        "      return(self.labels['labels'])   \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.location + self.image_paths[idx]\n",
        "        image = cv2.imread(image_filepath) \n",
        "        label = self.image_paths[idx]\n",
        "\n",
        "        return image, label\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phyBWAEctbkg"
      },
      "outputs": [],
      "source": [
        "#implementation by Balawejder, M. [3]\n",
        "class ConvBlock(nn.Module):\n",
        "    # Convolution Block with Conv2d layer, Batch Normalization and ReLU. Act is an activation function. \n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels : int,\n",
        "        out_channels : int,\n",
        "        kernel_size : int,\n",
        "        stride : int,\n",
        "        act = nn.ReLU(),\n",
        "        groups = 1,\n",
        "        bn = True,\n",
        "        bias = False     \n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # If k = 1 -> p = 0, k = 3 -> p = 1, k = 5, p = 2. \n",
        "        padding = kernel_size // 2 \n",
        "        self.c = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_channels) if bn else nn.Identity()\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.act(self.bn(self.c(x)))\n",
        "\n",
        "\n",
        "class SeBlock(nn.Module):\n",
        "    # Squeeze and Excitation Block. \n",
        "    def __init__(\n",
        "        self, \n",
        "        in_channels : int\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        C = in_channels\n",
        "        r = C // 4\n",
        "        self.globpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc1 = nn.Linear(C, r, bias=False)\n",
        "        self.fc2 = nn.Linear(r, C, bias=False)\n",
        "        self.relu = nn.ReLU() \n",
        "        self.hsigmoid = nn.Hardsigmoid()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x shape: [N, C, H, W].  \n",
        "        f = self.globpool(x)\n",
        "        f = torch.flatten(f,1)\n",
        "        f = self.relu(self.fc1(f))\n",
        "        f = self.hsigmoid(self.fc2(f))\n",
        "        f = f[:,:,None,None]\n",
        "        # f shape: [N, C, 1, 1]  \n",
        "\n",
        "        scale = x * f\n",
        "        return scale\n",
        "\n",
        "# BNeck\n",
        "class BNeck(nn.Module):\n",
        "    # MobileNetV3 Block \n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels : int,\n",
        "        out_channels : int,\n",
        "        kernel_size : int, \n",
        "        exp_size : int,\n",
        "        se : bool, \n",
        "        act : torch.nn.modules.activation,\n",
        "        stride : int\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.add = in_channels == out_channels and stride == 1\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            ConvBlock(in_channels, exp_size, 1, 1, act),\n",
        "            ConvBlock(exp_size, exp_size, kernel_size, stride, act, exp_size),\n",
        "            SeBlock(exp_size) if se == True else nn.Identity(),\n",
        "            ConvBlock(exp_size, out_channels, 1, 1, act=nn.Identity())\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        res = self.block(x)\n",
        "        if self.add:\n",
        "            res = res + x\n",
        "\n",
        "        return res\n",
        "\n",
        "\"\"\" MobileNetV3 \"\"\"\n",
        "class MobileNetV3(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config_name : str,\n",
        "        in_channels = 3,\n",
        "        classes = 2\n",
        "        ):\n",
        "        super().__init__()\n",
        "        config = self.config(config_name)\n",
        "\n",
        "        # First convolution(conv2d) layer. \n",
        "        self.conv = ConvBlock(in_channels, 16, 3, 2, nn.Hardswish())\n",
        "        # Bneck blocks in a list. \n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for c in config:\n",
        "            kernel_size, exp_size, in_channels, out_channels, se, nl, s = c\n",
        "            self.blocks.append(BNeck(in_channels, out_channels, kernel_size, exp_size, se, nl, s))\n",
        "        \n",
        "        # Classifier \n",
        "        last_outchannel = config[-1][3]\n",
        "        last_exp = config[-1][1]\n",
        "        out = 1280 if config_name == \"large\" else 1024\n",
        "        self.classifier = nn.Sequential(\n",
        "            ConvBlock(last_outchannel, last_exp, 1, 1, nn.Hardswish()),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            ConvBlock(last_exp, out, 1, 1, nn.Hardswish(), bn=False, bias=True),\n",
        "            nn.Dropout(0.8),\n",
        "            nn.Conv2d(out, classes, 1, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.conv(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "    def config(self, name):\n",
        "        HE, RE = nn.Hardswish(), nn.ReLU()\n",
        "        # [kernel, exp size, in_channels, out_channels, SEBlock(SE), activation function(NL), stride(s)] \n",
        "        large = [\n",
        "                [3, 16, 16, 16, False, RE, 1],\n",
        "                [3, 64, 16, 24, False, RE, 2],\n",
        "                [3, 72, 24, 24, False, RE, 1],\n",
        "                [5, 72, 24, 40, True, RE, 2],\n",
        "                [5, 120, 40, 40, True, RE, 1],\n",
        "                [5, 120, 40, 40, True, RE, 1],\n",
        "                [3, 240, 40, 80, False, HE, 2],\n",
        "                [3, 200, 80, 80, False, HE, 1],\n",
        "                [3, 184, 80, 80, False, HE, 1],\n",
        "                [3, 184, 80, 80, False, HE, 1],\n",
        "                [3, 480, 80, 112, True, HE, 1],\n",
        "                [3, 672, 112, 112, True, HE, 1],\n",
        "                [5, 672, 112, 160, True, HE, 2],\n",
        "                [5, 960, 160, 160, True, HE, 1],\n",
        "                [5, 960, 160, 160, True, HE, 1]\n",
        "        ]\n",
        "\n",
        "        small = [\n",
        "                [3, 16, 16, 16, True, RE, 2],\n",
        "                [3, 72, 16, 24, False, RE, 2],\n",
        "                [3, 88, 24, 24, False, RE, 1],\n",
        "                [5, 96, 24, 40, True, HE, 2],\n",
        "                [5, 240, 40, 40, True, HE, 1],\n",
        "                [5, 240, 40, 40, True, HE, 1],\n",
        "                [5, 120, 40, 48, True, HE, 1],\n",
        "                [5, 144, 48, 48, True, HE, 1],\n",
        "                [5, 288, 48, 96, True, HE, 2],\n",
        "                [5, 576, 96, 96, True, HE, 1],\n",
        "                [5, 576, 96, 96, True, HE, 1]\n",
        "        ]\n",
        "\n",
        "        if name == \"large\": return large\n",
        "        if name == \"small\": return small\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipieline\n",
        "\n",
        "This section runs the actual predictions. First the YOLO model, then landmark predictions and image crops, and finally the food classification. "
      ],
      "metadata": {
        "id": "SiBxm3m00r2O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-yJbw-YtXNu"
      },
      "source": [
        "## Face Detection\n",
        "\n",
        "To view the selected images navigate to ./yolov5/runs/detect/pictures in your google drive. If any of the images look wrong you can delete the to remove them from further predictions. \n",
        "\n",
        "You can adjust the confidence value to save more or fewer images. With lower confidence the quality will decrease but more frames will be saved. \n",
        "\n",
        "Note: running the section of code will delete the previous predictions in pictures and crops folder. If you don't want this to happen comment out the section marked below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFuLXb1BtHXA"
      },
      "outputs": [],
      "source": [
        "#training and prediction code by Solawetz, J. [1]\n",
        "\n",
        "%cd ./yolov5\n",
        "\n",
        "#comment out these 4 lines of code if you don't want the images to be removed.\n",
        "!rm -rf ./runs/detect/pictures\n",
        "!rm -rf ./runs/detect/crops\n",
        "\n",
        "os.mkdir('./runs/detect/pictures')\n",
        "os.mkdir('./runs/detect/crops')\n",
        "\n",
        "#                                                                      prediction confidence levels     path to video folder\n",
        "!python detect.py --weights ../BlueTitDiet/models/aug_exp_v2.pt --img 416 --conf 0.4 --source ../gdrive/MyDrive/videos/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By running this code cell you can see the images selected and the number of visits detected."
      ],
      "metadata": {
        "id": "OM3hfM1JJDsc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9LrZqx52pgi",
        "outputId": "b1f34516-364e-4118-f7e5-e6a7ada1605f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Visit dictionary\n",
            "{'81_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '81'}, '82_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '82'}, '83_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '83'}, '84_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '84'}, '85_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '85'}, '86_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '86'}, '93_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '93'}, '94_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '94'}, '96_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '96'}, '98_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '98'}, '110_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '110'}, '111_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '111'}, '112_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '112'}, '113_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '113'}, '114_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '114'}, '116_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '116'}, '117_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '117'}, '122_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '122'}, '127_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '127'}, '128_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '128'}, '129_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '129'}, '130_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '130'}, '131_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 1, 'frame': '131'}, '965_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 2, 'frame': '965'}, '966_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 2, 'frame': '966'}, '967_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 2, 'frame': '967'}, '968_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 2, 'frame': '968'}, '969_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 2, 'frame': '969'}, '970_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 2, 'frame': '970'}, '971_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 2, 'frame': '971'}, '972_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 2, 'frame': '972'}, '1421_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1421'}, '1424_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1424'}, '1425_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1425'}, '1426_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1426'}, '1427_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1427'}, '1428_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1428'}, '1429_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1429'}, '1430_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1430'}, '1431_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1431'}, '1433_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1433'}, '1434_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1434'}, '1435_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1435'}, '1436_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1436'}, '1438_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 3, 'frame': '1438'}, '1461_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1461'}, '1462_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1462'}, '1463_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1463'}, '1464_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1464'}, '1465_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1465'}, '1467_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1467'}, '1476_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1476'}, '1477_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1477'}, '1478_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1478'}, '1479_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1479'}, '1480_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1480'}, '1481_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1481'}, '1482_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1482'}, '1483_2016-06-03 06-03-13-Q-M.AVI.png': {'visit': 4, 'frame': '1483'}}\n"
          ]
        }
      ],
      "source": [
        "# Read dictionary pkl file\n",
        "%cd ..\n",
        "with open('./yolov5/runs/detect/pictures/visits.pkl', 'rb') as fp:\n",
        "    visits = pickle.load(fp)\n",
        "    print('Visit dictionary')\n",
        "    print(visits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDVqPkhytZ_L"
      },
      "source": [
        "## Landmark Detection\n",
        "\n",
        "To view the cropped images go to ./yolov5/runs/detect/crops . Again, the results can be removed if any of the crops are wrong. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8f_rYFPuDYE"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = get_model(num_keypoints = 3, weights_path = './BlueTitDiet/models/keypointsrcnn_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymmbce8M3BLn"
      },
      "outputs": [],
      "source": [
        "crop_images('./yolov5/runs/detect/pictures/', './yolov5/runs/detect/crops/', 48, 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJyXAraEtdcl"
      },
      "source": [
        "## Insect Classifier\n",
        "\n",
        "This section loads the crops and makes prediction per each of the images. The visit dictionary is updated with preditions and then the most common value per visit is selected. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UB0fgBI5hSg",
        "outputId": "4bf1e091-b260-408d-cbd5-c8d1e9b7790d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1479_2016-06-03 06-03-13-Q-M.AVI.png', '122_2016-06-03 06-03-13-Q-M.AVI.png', '131_2016-06-03 06-03-13-Q-M.AVI.png', '1476_2016-06-03 06-03-13-Q-M.AVI.png', '970_2016-06-03 06-03-13-Q-M.AVI.png', '1421_2016-06-03 06-03-13-Q-M.AVI.png', '94_2016-06-03 06-03-13-Q-M.AVI.png', '1464_2016-06-03 06-03-13-Q-M.AVI.png', '1428_2016-06-03 06-03-13-Q-M.AVI.png', '130_2016-06-03 06-03-13-Q-M.AVI.png', '82_2016-06-03 06-03-13-Q-M.AVI.png', '1465_2016-06-03 06-03-13-Q-M.AVI.png', '1477_2016-06-03 06-03-13-Q-M.AVI.png', '83_2016-06-03 06-03-13-Q-M.AVI.png', '81_2016-06-03 06-03-13-Q-M.AVI.png', '1481_2016-06-03 06-03-13-Q-M.AVI.png', '96_2016-06-03 06-03-13-Q-M.AVI.png', '85_2016-06-03 06-03-13-Q-M.AVI.png', '1483_2016-06-03 06-03-13-Q-M.AVI.png', '128_2016-06-03 06-03-13-Q-M.AVI.png', '1480_2016-06-03 06-03-13-Q-M.AVI.png', '112_2016-06-03 06-03-13-Q-M.AVI.png', '1463_2016-06-03 06-03-13-Q-M.AVI.png', '1425_2016-06-03 06-03-13-Q-M.AVI.png', '1461_2016-06-03 06-03-13-Q-M.AVI.png', '1431_2016-06-03 06-03-13-Q-M.AVI.png', '116_2016-06-03 06-03-13-Q-M.AVI.png', '93_2016-06-03 06-03-13-Q-M.AVI.png', '117_2016-06-03 06-03-13-Q-M.AVI.png', '1429_2016-06-03 06-03-13-Q-M.AVI.png', '86_2016-06-03 06-03-13-Q-M.AVI.png', '1435_2016-06-03 06-03-13-Q-M.AVI.png', '129_2016-06-03 06-03-13-Q-M.AVI.png', '1436_2016-06-03 06-03-13-Q-M.AVI.png', '84_2016-06-03 06-03-13-Q-M.AVI.png', '1462_2016-06-03 06-03-13-Q-M.AVI.png', '1430_2016-06-03 06-03-13-Q-M.AVI.png', '113_2016-06-03 06-03-13-Q-M.AVI.png', '967_2016-06-03 06-03-13-Q-M.AVI.png', '966_2016-06-03 06-03-13-Q-M.AVI.png', '110_2016-06-03 06-03-13-Q-M.AVI.png', '1434_2016-06-03 06-03-13-Q-M.AVI.png', '114_2016-06-03 06-03-13-Q-M.AVI.png', '1482_2016-06-03 06-03-13-Q-M.AVI.png', '969_2016-06-03 06-03-13-Q-M.AVI.png', '1424_2016-06-03 06-03-13-Q-M.AVI.png', '1467_2016-06-03 06-03-13-Q-M.AVI.png', '1426_2016-06-03 06-03-13-Q-M.AVI.png', '968_2016-06-03 06-03-13-Q-M.AVI.png', '1438_2016-06-03 06-03-13-Q-M.AVI.png', '1433_2016-06-03 06-03-13-Q-M.AVI.png', '1478_2016-06-03 06-03-13-Q-M.AVI.png', '127_2016-06-03 06-03-13-Q-M.AVI.png', '1427_2016-06-03 06-03-13-Q-M.AVI.png', '98_2016-06-03 06-03-13-Q-M.AVI.png', '111_2016-06-03 06-03-13-Q-M.AVI.png']\n"
          ]
        }
      ],
      "source": [
        "# list to store files\n",
        "test_img = []\n",
        "test_path = './yolov5/runs/detect/crops/'\n",
        "\n",
        "# Iterate directory\n",
        "for path in os.listdir(test_path):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join(test_path, path)):\n",
        "        test_img.append(path)\n",
        "print(test_img)\n",
        "\n",
        "test_dataset = ClassDataset(test_img, np.zeros(len(test_img)), test_path)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7F68wff4wLq",
        "outputId": "a5bb6bed-4dd8-44a2-8cf5-654cf07756bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n"
          ]
        }
      ],
      "source": [
        "name = \"large\"\n",
        "rho = 1\n",
        "res = int(rho * 45)\n",
        "\n",
        "PATH = './BlueTitDiet/models/net_2class_valid_test_balanced.pth'\n",
        "net = MobileNetV3(name)\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "print(net(torch.rand(1, 3, res, res)).shape)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRijQp2B40_5"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.to(torch.float)\n",
        "images = images.permute(0,3,1,2)\n",
        "\n",
        "outputs = net(images)\n",
        "prob = nnf.softmax(outputs, dim=1)\n",
        "top_p, top_class = prob.topk(1, dim = 1)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "_, predicted = torch.max(outputs, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell will run the voting system and print out visit numbers, what food was brought and the time in the video the visit occurs. "
      ],
      "metadata": {
        "id": "Vzl7CDxbJUJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNpBG11V8F26",
        "outputId": "94665ae8-3310-4357-f6ae-7e9409ae0dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visit:  1 food:  insect time:  0:00:06.750000\n",
            "visit:  2 food:  caterpillar time:  0:01:20.500000\n",
            "visit:  3 food:  caterpillar time:  0:01:58.416667\n",
            "visit:  4 food:  caterpillar time:  0:02:01.750000\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(predicted)):\n",
        "  visits[labels[i]]['food'] = int(predicted[i])\n",
        "\n",
        "vote = {}\n",
        "for item in visits.items():\n",
        "  visit = item[1]['visit']\n",
        "  if 'food' in item[1]:\n",
        "    food = item[1]['food']\n",
        "    if visit in vote.keys(): \n",
        "      vote[visit]['food'].append(food)\n",
        "    else:\n",
        "      vote[visit] = {'food' : [food], 'frame': item[1]['frame']}\n",
        "\n",
        "for key in vote.keys():\n",
        "    if most_common(vote[key]['food']) == 0:\n",
        "      food_item = 'caterpillar'\n",
        "    elif most_common(vote[key]['food']) == 1:\n",
        "      food_item = 'insect'\n",
        "    else:\n",
        "      food_item = 'other'\n",
        "\n",
        "    td = timedelta(seconds=(int(vote[key]['frame']) / 12))\n",
        "    print('visit: ',  str(key) , 'food: ' ,food_item, 'time: ', td )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## References\n",
        "\n",
        " 1. Solawetz, J. and Nelson, J. (2020) How to train yolov5 on a custom dataset, Roboflow Blog. Roboflow Blog. Available at: https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/ (Accessed: April 12, 2023). \n",
        "\n",
        " 2. P, A. (2022) How to train a custom keypoint detection model with pytorch, Medium. Medium. Available at: https://medium.com/@alexppppp/how-to-train-a-custom-keypoint-detection-model-with-pytorch-d9af90e111da (Accessed: April 12, 2023). \n",
        "\n",
        " 3. Balawejder, M. (2022) Mobilenetv3 , GitHub. Available at: https://github.com/maciejbalawejder/Deep-Learning-Collection/tree/main/ConvNets/MobileNetV3 (Accessed: April 12, 2023). \n",
        "\n"
      ],
      "metadata": {
        "id": "sam4_UZdFLBW"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
